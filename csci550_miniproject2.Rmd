---
title: "CSCI 550 Advanced Data Mining miniproject2"
author: "Stephen Huysman, Connor Nelle, Bryan Portillo, Behzad Karimi"
output:
  html_document:
    toc: true
---

```{r packages}
library(GGally)
library(janitor)
library(corrplot)

library(ggfortify) ## PCA Plots
library(glmnet) ## Penalized Regression
library(MASS) ## Step Selection
library(MuMIn) ## Compare Model Performance
library(caret)
library(splines)
library(factoextra)
library(tidyverse)

set.seed(123)
```

```{r load-data}
house_prices <- read_csv("./cook_county_train_val.csv")

house_prices <- house_prices %>% clean_names()

# Drop meaningless variables
house_prices <- house_prices %>% select(-x1, -pin, -description)

# These columns only have one level per factor, so are not useful for modeling
house_prices <- house_prices %>%
  select(-use, -modeling_group)

# These are duplicates or modifications of other columns
house_prices <- house_prices %>%
  select(-neigborhood_code_mapping, -town_code, -neighborhood_code)

# Documentation says other improvements is not clean enough to be useful
house_prices <- house_prices %>%
  select(-other_improvements)

# Clean sale prices = 1
house_prices <- house_prices %>%
  filter(sale_price > 1)

# Codebook specifies that houses with missing age are set to 10.  Since we can't tell houses with missing age vs houses with true age 10, we decided to drop data points with age == 10.
house_prices <- house_prices %>%
  filter(age != 10)

# land_square_feet and lot_size are the same so we drop one
house_prices <- house_prices %>%
  select(-land_square_feet)

# there are lots of different fields treating date differently, for example sale month of year, sale half year, etc.  To cut down on redundant predictors, we are keeping sale year and sale month of year, as these fields contain the most information
house_prices <- house_prices %>%
  select(-sale_quarter, -sale_half_year, -sale_quarter_of_year, -sale_half_of_year)

# deed number is a unique ID that appears randomly assigned
house_prices <- house_prices %>%
  select(-deed_no)

# age_decade is just age / 10
house_prices <- house_prices %>%
  select(-age_decade)

# lots of zeroes for estimate_land and estimate_building.  We are interested in how good of a predictor of sale prices these estimates are, so we are dropping observations where these are 0
house_prices <- house_prices %>%
  filter(estimate_land != 0, estimate_building != 0)


# apartments ranges from 0 to 6, but there are no observations with 1 apartment.  We want to treat this variable as continuous, so we are transforming apartments = 0 to 1.  This means that an observation with 1 apartment is a free standing dwelling, 2+ apartments is an apartment complex.
house_prices <- house_prices %>%
  mutate(apartments = if_else(apartments == 0, 1, apartments))


house_prices <- house_prices %>%
  mutate_at(vars(property_class,
                 wall_material,
                 roof_material,
                 basement,
                 basement_finish,
                 central_heating,
                 other_heating,
                 central_air,
                 attic_type,
                 attic_finish,
                 design_plan,
                 cathedral_ceiling,
                 construction_quality,
                 site_desirability,
                 garage_1_material,
                 garage_1_attachment,
                 garage_1_area,
                 garage_2_material,
                 garage_2_attachment,
                 garage_2_area,
                 porch,
                 repair_condition,
                 multi_code,
                 census_tract,
                 multi_property_indicator,
                 o_hare_noise,
                 floodplain,
                 road_proximity,
                 pure_market_filter,
                 garage_indicator,
                 town_and_neighborhood,
                 sale_month_of_year),
            factor)

input_matrix <- model.matrix(sale_price ~ -1 + ., data = house_prices)
```

```{r eda, cache = TRUE}
for (var in c("building_square_feet", "lot_size", "estimate_land", "estimate_building", "age")) {
  print(var)
  plot <- ggplot(house_prices, aes(x = .data[[var]], y = log(sale_price))) +
    geom_point() +
    ggtitle(paste(var, "vs log Sale Price")) 
  print(plot)
}

ggplot(house_prices, aes(x = town_and_neighborhood, y = log(sale_price))) +
  geom_boxplot() +
  ggtitle("Town and Neighborhood vs log Sale Price")

ggplot(house_prices, aes(x = property_class, y = log(sale_price))) +
  geom_boxplot() +
  ggtitle("Property Class vs log Sale Price")

M <- cor(select_if(house_prices, is.numeric))
corrplot(M)

summary(house_prices) %>%
  knitr::kable()
```

# Linear Models
```{r linear-models}

baseline <- lm(sale_price ~ building_square_feet, data = house_prices)

summary(baseline)

null_model <- lm(sale_price ~ 1, data = house_prices)
```

```{r full_model, eval = F}
## This takes a very long time to run, so save it to disk.  The file takes ~2.6 GB
full_model <- lm(sale_price ~ ., data = house_prices)
saveRDS(full_model, "full_model.RDS")
```

```{r full_model2}
full_model <- readRDS("full_model.RDS")
summary(full_model)
```

# Subset Selection
``` {r subset-selection, eval = F}
## Code below used to generate forward step fit
## This takes forever to run without limited steps.  So we had to limit to 5 steps and save to disk.  The RDS file takes ~1GB
step_fit <- stepAIC(null_model,  scope = list(lower = null_model, upper = full_model), direction = "forward", steps = 5)
saveRDS(step_fit, "step_fit.RDS")
```

``` {r forward-selection-load}
## Load cached step fit model
step_fit <- readRDS("step_fit.RDS")

summary(step_fit)
```

# L_1/L_2 Regularized Regression

``` {r ridge-lasso, cache = TRUE}
lasso.cv <- cv.glmnet(x = input_matrix,
                      y = house_prices$sale_price,
                      alpha = 1,
                      nfolds = 5,
                      parallel = TRUE)

plot(lasso.cv)

plot(lasso.cv$glmnet.fit)

coef(lasso.cv, s = "lambda.min")

## Lambda min
lasso.cv$lambda.min
lasso.cv$lambda.1se

ridge.cv <- cv.glmnet(x = input_matrix,
                      y = house_prices$sale_price,
                      alpha = 0,
                      nfolds = 5,
                      parallel = TRUE)

plot(ridge.cv)

plot(ridge.cv$glmnet.fit)

coef(ridge.cv, s = "lambda.min")

## Lambda min
ridge.cv$lambda.min
ridge.cv$lambda.1se

```


# PCA

``` {r pca, eval = F}
# Takes ages to run
pca <- prcomp(input_matrix, scale. = TRUE)

autoplot(pca,
         data = house_prices,
         color = "age",
         loadings = TRUE,
         loadings.color = "blue",
         loadings.label = TRUE,
         loadings.label.size = 2) +
  guides(col="none")

fviz_eig(pca)
```

# Non-linear models

```{r natural-spline}
ggplot(house_prices, aes(x = building_square_feet, y = log(sale_price))) +
  geom_point() +
  ggtitle("Building square feet vs log(sale_price)")

### Generated with Claude 3.5 Sonnet
cv_natural_spline <- function(data, response, predictor, df = 3, k = 5) {
  # Create fold indices
  set.seed(123)  # for reproducibility
  folds <- createFolds(data[[response]], k = k, list = TRUE)
  
  # Store MSE for each fold
  mse_values <- numeric(k)
  
  # Perform k-fold CV
  for (i in seq_along(folds)) {
    # Split data into training and test
    test_indices <- folds[[i]]
    train_data <- data[-test_indices, ]
    test_data <- data[test_indices, ]
    
    # Fit natural spline on training data
    formula <- as.formula(paste(response, "~ ns(", predictor, ", df =", df, ")"))
    model <- lm(formula, data = train_data)
    
    # Make predictions on test data
    predictions <- predict(model, newdata = test_data)
    
    # Calculate MSE for this fold
    mse_values[i] <- mean((test_data[[response]] - predictions)^2)
  }
  
  # Return results
  list(
    fold_mse = mse_values,
    mean_mse = mean(mse_values),
    sd_mse = sd(mse_values)
  )
}

df_values <- 1:10
results <- lapply(df_values, function(df) {
  cv_result <- cv_natural_spline(data = house_prices, "sale_price", "building_square_feet", df = df)
  c(df = df, mean_mse = cv_result$mean_mse)
})

# Convert results to data frame
results_df <- data.frame(do.call(rbind, results))

# Find best df
best_df <- results_df$df[which.min(results_df$mean_mse)]

# Fit final model with best df
final_formula <- as.formula(paste("sale_price ~ ns(building_square_feet, df =", best_df, ")"))
(final_model <- lm(final_formula, data = house_prices))

# Plot results
plot(house_prices$building_square_feet, log(house_prices$sale_price), main = "Natural Spline Fit with Best df", 
     xlab = "building_square_feet", ylab = "log(sale_price)", pch = 16, col = "gray")
x_new <- seq(min(house_prices$building_square_feet), max(house_prices$building_square_feet), length.out = 100)
pred <- log(predict(final_model, newdata = data.frame(building_square_feet = x_new)))
lines(x_new, pred, col = "red", lwd = 2)

```

``` {r polynomial}

# Function to perform CV for polynomial regression
### Generated with Claude 3.5 Sonnet
cv_polynomial <- function(data, response, predictor, max_degree = 10, k = 5) {
  # Create fold indices
  set.seed(123)  # for reproducibility
  folds <- createFolds(data[[response]], k = k, list = TRUE)
  
  # Store results for each degree
  results <- data.frame(
    degree = 1:max_degree,
    mean_mse = NA,
    sd_mse = NA
  )
  
  # Test each polynomial degree
  for (degree in 1:max_degree) {
    mse_values <- numeric(k)
    
    # Perform k-fold CV
    for (i in seq_along(folds)) {
      # Split data into training and test
      test_indices <- folds[[i]]
      train_data <- data[-test_indices, ]
      test_data <- data[test_indices, ]
      
      # Create polynomial terms
      poly_formula <- as.formula(
        paste(response, "~ poly(", predictor, ", degree =", degree, ", raw = TRUE)")
      )
      
      # Fit model
      model <- lm(poly_formula, data = train_data)
      
      # Make predictions
      predictions <- predict(model, newdata = test_data)
      
      # Calculate MSE for this fold
      mse_values[i] <- mean((test_data[[response]] - predictions)^2)
    }
    
    # Store results for this degree
    results$mean_mse[degree] <- mean(mse_values)
    results$sd_mse[degree] <- sd(mse_values)
  }
  
  return(results)
}

# Perform cross-validation
cv_results <- cv_polynomial(house_prices, "sale_price", "building_square_feet", max_degree = 10)

# Find best degree
best_degree <- which.min(cv_results$mean_mse)

# Fit final model with best degree
final_formula <- as.formula(
  paste("sale_price ~ poly(building_square_feet, degree =", best_degree, ", raw = TRUE)")
)
final_model <- lm(final_formula, data = house_prices)

# Create plots
# 1. Cross-validation results
cv_plot <- ggplot(cv_results, aes(x = degree, y = mean_mse)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(ymin = mean_mse - sd_mse, 
                    ymax = mean_mse + sd_mse),
                width = 0.2) +
  geom_vline(xintercept = best_degree, linetype = "dashed", color = "red") +
  labs(title = "Cross-validation Results by Polynomial Degree",
       x = "Polynomial Degree",
       y = "Mean Square Error") +
  theme_minimal()

# 2. Final fit plot
grid_x <- seq(min(house_prices$building_square_feet), max(house_prices$building_square_feet), length.out = 200)
grid_data <- data.frame(building_square_feet = grid_x)
predictions <- predict(final_model, newdata = grid_data)

fit_plot <- ggplot() +
  geom_point(data = house_prices, aes(x = building_square_feet, y = log(sale_price)), alpha = 0.5) +
  geom_line(data = data.frame(x = grid_x, y = log(predictions)),
            aes(x = x, y = y), color = "red", size = 1) +
  labs(title = paste("Best Polynomial Fit (Degree =", best_degree, ")"),
       x = "x",
       y = "y") +
  theme_minimal()

# Print results
print("Cross-validation results:")
print(cv_results)
print(paste("\nBest polynomial degree:", best_degree))
print("\nSummary of final model:")
print(summary(final_model))

# Display plots
print(cv_plot)
print(fit_plot)

```

``` {r smooth-spline}
# Function to perform CV for smooth splines
# Generated with Claude 3.5 Sonnet
cv_smooth_spline <- function(data, response, predictor, spar = NULL, k = 5) {
  # Create fold indices
  set.seed(123)  # for reproducibility
  folds <- createFolds(data[[response]], k = k, list = TRUE)
  
  # Store MSE for each fold
  mse_values <- numeric(k)
  
  # Perform k-fold CV
  for (i in seq_along(folds)) {
    # Split data into training and test
    test_indices <- folds[[i]]
    train_data <- data[-test_indices, ]
    test_data <- data[test_indices, ]
    
    # Fit smooth spline on training data
    model <- smooth.spline(
      x = train_data[[predictor]], 
      y = train_data[[response]],
      spar = spar  # NULL means GCV is used to select smoothing parameter
    )
    
    # Make predictions on test data
    predictions <- predict(model, test_data[[predictor]])$y
    
    # Calculate MSE for this fold
    mse_values[i] <- mean((test_data[[response]] - predictions)^2)
  }
  
  # Return results
  list(
    fold_mse = mse_values,
    mean_mse = mean(mse_values),
    sd_mse = sd(mse_values),
    spar = spar
  )
}

# Try different smoothing parameters
spar_values <- c(0.3, 0.5, 0.7, 0.9, 0.99, NULL)  # NULL means use GCV
results <- lapply(spar_values, function(spar) {
  cv_result <- cv_smooth_spline(house_prices, "sale_price", "building_square_feet", spar = spar)
  c(spar = if(is.null(spar)) "GCV" else spar, 
    mean_mse = cv_result$mean_mse)
})

# Convert results to data frame
results_df <- data.frame(do.call(rbind, results))

# Find best spar value
best_result <- results_df[which.min(results_df$mean_mse), ]
best_spar <- if(best_result$spar == "GCV") NULL else as.numeric(best_result$spar)

# Fit final model with best smoothing parameter
smooth_model <- smooth.spline(house_prices$building_square_feet, log(house_prices$sale_price), spar = best_spar)

# Plot results
plot(house_prices$building_square_feet, log(house_prices$sale_price), main = paste("Smooth Spline Fit with", 
                       if(is.null(best_spar)) "GCV" else paste("spar =", best_spar)), 
     xlab = "x", ylab = "y", pch = 16, col = "gray")
lines(smooth_model, col = "red", lwd = 2)

# Print cross-validation results
print(results_df)

```

# Compare model performance

```{r model-performance}
### AICc for glmnet models
### https://stackoverflow.com/questions/63171921/is-there-a-way-in-r-to-determine-aic-from-cv-glmnet
glmnet_cv_aicc <- function(fit, lambda = 'lambda.min'){
  whlm <- which(fit$lambda == fit[[lambda]])
  with(fit$glmnet.fit,
       {
         tLL <- nulldev - nulldev * (1 - dev.ratio)[whlm]
         k <- df[whlm]
         n <- nobs
         return(list('AIC' = -2 * tLL + 2 * k,
                     'BIC' = log(n) * k - tLL))
       })
}

## aic for smooth spline, generated by Claude 3.5 Sonnet
get_smooth_spline_aicc <- function(fit) {
  # Get number of observations
  n <- length(fit$x)
  
  # Get effective degrees of freedom
  df <- fit$df
  
  # Get residual sum of squares
  rss <- sum((fit$y - fit$yin)^2)
  
  # Calculate sigma^2 (error variance)
  sigma2 <- rss / (n - df)
  
  # Calculate log-likelihood (assuming Gaussian errors)
  loglik <- -n/2 * log(2 * pi * sigma2) - rss/(2 * sigma2)
  
  # Calculate AIC
  aic <- -2 * loglik + 2 * df
  
  # Calculate AICc (corrected for small sample size)
  aicc <- aic + (2 * df * (df + 1))/(n - df - 1)
  
  return(list(
    AIC = aic,
    AICc = aicc,
    df = df,
    n = n,
    RSS = rss,
    loglik = loglik
  ))
}

```


| Model                        | Formula                                                                                                                      | AIC                              |
|------------------------------|------------------------------------------------------------------------------------------------------------------------------|-----------------------------------|
| Baseline                     | sale_price ~ building_square_feet                                                                                            | `r AIC(baseline)`                |
| Null                         | sale_price ~ 1                                                                                                               | `r AIC(null_model)`              |
| Full                         | sale_price ~ .                                                                                                               | `r AIC(full_model)`              |
| Step fit (forward selection) | sale_price ~ estimate_building + town_and_neighborhood + building_square_feet +	multi_property_indicator + property_class | `r AIC(step_fit)`                |
| Lasso                        |                                                                                                                              | `r glmnet_cv_aicc(lasso.cv)$AIC` |
| Ridge                        |                                                                                                                              | `r glmnet_cv_aicc(ridge.cv)$AIC` |
| Smooth Spline                | sale_price ~ smooth.spline(building_square_feet, spar = .99                                                                                | `r get_smooth_spline_aicc(smooth_model)$AIC`             |
| Polynomial                | sale_price ~ poly(building_square_feet, 4)                                                                              | `r AIC(poly_fit)`             |
